{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Trading based on RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def setup_seed(seed):\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed_all(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     random.seed(seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "# # 设置随机数种子\n",
    "# setup_seed(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading data and data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asset_returns tensor([ 1.7000e-01, -7.0600e+00,  4.4730e+01,  ..., -6.4793e+02,\n",
      "         8.9026e+02,  1.6336e+03])\n"
     ]
    }
   ],
   "source": [
    "raw_prices = pd.read_csv(\"BTC.csv\", header=None)\n",
    "prices = np.array(raw_prices[1])\n",
    "OFFSET = 100 #数据起始点\n",
    "M = 10  #输入网络的历史窗口的大小，用于在每个时间步更新权重的历史大小,r的系数个数\n",
    "T = 2000 #交易者输入的时间序列长度\n",
    "N = 600  #验证集大小\n",
    "prices = prices[OFFSET:OFFSET+M+T+N+1]\n",
    "# asset_returns = torch.tensor(np.log(prices[1:]) - np.log(prices[:-1])).to(torch.float32)\n",
    "asset_returns = torch.tensor(prices[1:] - prices[:-1]).to(torch.float32)\n",
    "print('asset_returns',asset_returns)\n",
    "scaler = StandardScaler()\n",
    "normalized_asset_returns = torch.tensor(scaler.fit_transform(asset_returns[:M+T][:, None])[:, 0]).to(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RRL(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(RRL, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().cuda()\n",
    "        out, (hn) = self.rnn(x, (h0.detach()))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return torch.tanh(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RRL(\n",
      "  (rnn): RNN(1, 64, num_layers=3, batch_first=True)\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_dim = 1\n",
    "hidden_dim = 64\n",
    "num_layers = 3\n",
    "output_dim = 1\n",
    "use_gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n",
    "model = RRL(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "rewards = []\n",
    "max_iter = 1000\n",
    "miu = 1\n",
    "delta = 0.04\n",
    "eps = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 Sharpe:  tensor(-0.1133)\n",
      "Epoch  1 Sharpe:  tensor(0.0282)\n",
      "Epoch  2 Sharpe:  tensor(0.2033)\n",
      "Epoch  3 Sharpe:  tensor(0.2524)\n",
      "Epoch  4 Sharpe:  tensor(0.2971)\n",
      "Epoch  5 Sharpe:  tensor(0.3424)\n",
      "Epoch  6 Sharpe:  tensor(0.3895)\n",
      "Epoch  7 Sharpe:  tensor(0.4378)\n",
      "Epoch  8 Sharpe:  tensor(0.4857)\n",
      "Epoch  9 Sharpe:  tensor(0.5311)\n",
      "Epoch  10 Sharpe:  tensor(0.5732)\n",
      "Epoch  11 Sharpe:  tensor(0.6131)\n",
      "Epoch  12 Sharpe:  tensor(0.6528)\n",
      "Epoch  13 Sharpe:  tensor(0.6946)\n",
      "Epoch  14 Sharpe:  tensor(0.7403)\n",
      "Epoch  15 Sharpe:  tensor(0.7903)\n",
      "Epoch  16 Sharpe:  tensor(0.8435)\n",
      "Epoch  17 Sharpe:  tensor(0.8979)\n",
      "Epoch  18 Sharpe:  tensor(0.9523)\n",
      "Epoch  19 Sharpe:  tensor(1.0079)\n",
      "Epoch  20 Sharpe:  tensor(1.0669)\n",
      "Epoch  21 Sharpe:  tensor(1.1298)\n",
      "Epoch  22 Sharpe:  tensor(1.1935)\n",
      "Epoch  23 Sharpe:  tensor(1.2527)\n",
      "Epoch  24 Sharpe:  tensor(1.3040)\n",
      "Epoch  25 Sharpe:  tensor(1.3496)\n",
      "Epoch  26 Sharpe:  tensor(1.3952)\n",
      "Epoch  27 Sharpe:  tensor(1.4447)\n",
      "Epoch  28 Sharpe:  tensor(1.4968)\n",
      "Epoch  29 Sharpe:  tensor(1.5479)\n",
      "Epoch  30 Sharpe:  tensor(1.5962)\n",
      "Epoch  31 Sharpe:  tensor(1.6441)\n",
      "Epoch  32 Sharpe:  tensor(1.6935)\n",
      "Epoch  33 Sharpe:  tensor(1.7422)\n",
      "Epoch  34 Sharpe:  tensor(1.7850)\n",
      "Epoch  35 Sharpe:  tensor(1.8219)\n",
      "Epoch  36 Sharpe:  tensor(1.8587)\n",
      "Epoch  37 Sharpe:  tensor(1.8970)\n",
      "Epoch  38 Sharpe:  tensor(1.9350)\n",
      "Epoch  39 Sharpe:  tensor(1.9728)\n",
      "Epoch  40 Sharpe:  tensor(2.0113)\n",
      "Epoch  41 Sharpe:  tensor(2.0490)\n",
      "Epoch  42 Sharpe:  tensor(2.0824)\n",
      "Epoch  43 Sharpe:  tensor(2.1122)\n",
      "Epoch  44 Sharpe:  tensor(2.1425)\n",
      "Epoch  45 Sharpe:  tensor(2.1750)\n",
      "Epoch  46 Sharpe:  tensor(2.2069)\n",
      "Epoch  47 Sharpe:  tensor(2.2363)\n",
      "Epoch  48 Sharpe:  tensor(2.2666)\n",
      "Epoch  49 Sharpe:  tensor(2.2999)\n",
      "Epoch  50 Sharpe:  tensor(2.3335)\n",
      "Epoch  51 Sharpe:  tensor(2.3659)\n",
      "Epoch  52 Sharpe:  tensor(2.3984)\n",
      "Epoch  53 Sharpe:  tensor(2.4304)\n",
      "Epoch  54 Sharpe:  tensor(2.4605)\n",
      "Epoch  55 Sharpe:  tensor(2.4882)\n",
      "Epoch  56 Sharpe:  tensor(2.5142)\n",
      "Epoch  57 Sharpe:  tensor(2.5388)\n",
      "Epoch  58 Sharpe:  tensor(2.5602)\n",
      "Epoch  59 Sharpe:  tensor(2.5786)\n",
      "Epoch  60 Sharpe:  tensor(2.5960)\n",
      "Epoch  61 Sharpe:  tensor(2.6118)\n",
      "Epoch  62 Sharpe:  tensor(2.6267)\n",
      "Epoch  63 Sharpe:  tensor(2.6418)\n",
      "Epoch  64 Sharpe:  tensor(2.6573)\n",
      "Epoch  65 Sharpe:  tensor(2.6724)\n",
      "Epoch  66 Sharpe:  tensor(2.6867)\n",
      "Epoch  67 Sharpe:  tensor(2.7006)\n",
      "Epoch  68 Sharpe:  tensor(2.7138)\n",
      "Epoch  69 Sharpe:  tensor(2.7266)\n",
      "Epoch  70 Sharpe:  tensor(2.7395)\n",
      "Epoch  71 Sharpe:  tensor(2.7534)\n",
      "Epoch  72 Sharpe:  tensor(2.7705)\n",
      "Epoch  73 Sharpe:  tensor(2.7884)\n",
      "Epoch  74 Sharpe:  tensor(2.8052)\n",
      "Epoch  75 Sharpe:  tensor(2.8227)\n",
      "Epoch  76 Sharpe:  tensor(2.8384)\n",
      "Epoch  77 Sharpe:  tensor(2.8561)\n",
      "Epoch  78 Sharpe:  tensor(2.8717)\n",
      "Epoch  79 Sharpe:  tensor(2.8856)\n",
      "Epoch  80 Sharpe:  tensor(2.8980)\n",
      "Epoch  81 Sharpe:  tensor(2.9088)\n",
      "Epoch  82 Sharpe:  tensor(2.9175)\n",
      "Epoch  83 Sharpe:  tensor(2.9258)\n",
      "Epoch  84 Sharpe:  tensor(2.9327)\n",
      "Epoch  85 Sharpe:  tensor(2.9396)\n",
      "Epoch  86 Sharpe:  tensor(2.9476)\n",
      "Epoch  87 Sharpe:  tensor(2.9587)\n",
      "Epoch  88 Sharpe:  tensor(2.9806)\n",
      "Epoch  89 Sharpe:  tensor(3.0032)\n",
      "Epoch  90 Sharpe:  tensor(3.0153)\n",
      "Epoch  91 Sharpe:  tensor(3.0226)\n",
      "Epoch  92 Sharpe:  tensor(3.0282)\n",
      "Epoch  93 Sharpe:  tensor(3.0378)\n",
      "Epoch  94 Sharpe:  tensor(3.0411)\n",
      "Epoch  95 Sharpe:  tensor(3.0493)\n",
      "Epoch  96 Sharpe:  tensor(3.0546)\n",
      "Epoch  97 Sharpe:  tensor(3.0603)\n",
      "Epoch  98 Sharpe:  tensor(3.0633)\n",
      "Epoch  99 Sharpe:  tensor(3.0688)\n",
      "Epoch  100 Sharpe:  tensor(3.0719)\n",
      "Epoch  101 Sharpe:  tensor(3.0767)\n",
      "Epoch  102 Sharpe:  tensor(3.0807)\n",
      "Epoch  103 Sharpe:  tensor(3.0840)\n",
      "Epoch  104 Sharpe:  tensor(3.0883)\n",
      "Epoch  105 Sharpe:  tensor(3.0926)\n",
      "Epoch  106 Sharpe:  tensor(3.0962)\n",
      "Epoch  107 Sharpe:  tensor(3.1010)\n",
      "Epoch  108 Sharpe:  tensor(3.1061)\n",
      "Epoch  109 Sharpe:  tensor(3.1146)\n",
      "Epoch  110 Sharpe:  tensor(3.1259)\n",
      "Epoch  111 Sharpe:  tensor(3.1367)\n",
      "Epoch  112 Sharpe:  tensor(3.1607)\n",
      "Epoch  113 Sharpe:  tensor(3.1654)\n",
      "Epoch  114 Sharpe:  tensor(3.1675)\n",
      "Epoch  115 Sharpe:  tensor(3.1992)\n",
      "Epoch  116 Sharpe:  tensor(3.1584)\n",
      "Epoch  117 Sharpe:  tensor(3.1762)\n",
      "Epoch  118 Sharpe:  tensor(3.1050)\n",
      "Epoch  119 Sharpe:  tensor(3.1894)\n",
      "Epoch  120 Sharpe:  tensor(3.2004)\n",
      "Epoch  121 Sharpe:  tensor(3.2222)\n",
      "Epoch  122 Sharpe:  tensor(3.2208)\n",
      "Epoch  123 Sharpe:  tensor(3.2449)\n",
      "Epoch  124 Sharpe:  tensor(3.2683)\n",
      "Epoch  125 Sharpe:  tensor(3.2811)\n",
      "Epoch  126 Sharpe:  tensor(3.3091)\n",
      "Epoch  127 Sharpe:  tensor(3.3365)\n",
      "Epoch  128 Sharpe:  tensor(3.3612)\n",
      "Epoch  129 Sharpe:  tensor(3.3980)\n",
      "Epoch  130 Sharpe:  tensor(3.4014)\n",
      "Epoch  131 Sharpe:  tensor(3.4088)\n",
      "Epoch  132 Sharpe:  tensor(3.4257)\n",
      "Epoch  133 Sharpe:  tensor(3.4418)\n",
      "Epoch  134 Sharpe:  tensor(3.4475)\n",
      "Epoch  135 Sharpe:  tensor(3.4516)\n",
      "Epoch  136 Sharpe:  tensor(3.4538)\n",
      "Epoch  137 Sharpe:  tensor(3.4522)\n",
      "Epoch  138 Sharpe:  tensor(3.4565)\n",
      "Epoch  139 Sharpe:  tensor(3.4626)\n",
      "Epoch  140 Sharpe:  tensor(3.4716)\n",
      "Epoch  141 Sharpe:  tensor(3.4779)\n",
      "Epoch  142 Sharpe:  tensor(3.4846)\n",
      "Epoch  143 Sharpe:  tensor(3.4880)\n",
      "Epoch  144 Sharpe:  tensor(3.4904)\n",
      "Epoch  145 Sharpe:  tensor(3.4943)\n",
      "Epoch  146 Sharpe:  tensor(3.4998)\n",
      "Epoch  147 Sharpe:  tensor(3.5060)\n",
      "Epoch  148 Sharpe:  tensor(3.5106)\n",
      "Epoch  149 Sharpe:  tensor(3.5159)\n",
      "Epoch  150 Sharpe:  tensor(3.5244)\n",
      "Epoch  151 Sharpe:  tensor(3.5313)\n",
      "Epoch  152 Sharpe:  tensor(3.5366)\n",
      "Epoch  153 Sharpe:  tensor(3.5449)\n",
      "Epoch  154 Sharpe:  tensor(3.5500)\n",
      "Epoch  155 Sharpe:  tensor(3.5527)\n",
      "Epoch  156 Sharpe:  tensor(3.5593)\n",
      "Epoch  157 Sharpe:  tensor(3.5624)\n",
      "Epoch  158 Sharpe:  tensor(3.5644)\n",
      "Epoch  159 Sharpe:  tensor(3.5681)\n",
      "Epoch  160 Sharpe:  tensor(3.5696)\n",
      "Epoch  161 Sharpe:  tensor(3.5711)\n",
      "Epoch  162 Sharpe:  tensor(3.5746)\n",
      "Epoch  163 Sharpe:  tensor(3.5787)\n",
      "Epoch  164 Sharpe:  tensor(3.5826)\n",
      "Epoch  165 Sharpe:  tensor(3.5880)\n",
      "Epoch  166 Sharpe:  tensor(3.5918)\n",
      "Epoch  167 Sharpe:  tensor(3.5951)\n",
      "Epoch  168 Sharpe:  tensor(3.5985)\n",
      "Epoch  169 Sharpe:  tensor(3.6007)\n",
      "Epoch  170 Sharpe:  tensor(3.6033)\n",
      "Epoch  171 Sharpe:  tensor(3.6060)\n",
      "Epoch  172 Sharpe:  tensor(3.6092)\n",
      "Epoch  173 Sharpe:  tensor(3.6117)\n",
      "Epoch  174 Sharpe:  tensor(3.6171)\n",
      "Epoch  175 Sharpe:  tensor(3.6429)\n",
      "Epoch  176 Sharpe:  tensor(3.6428)\n",
      "Epoch  177 Sharpe:  tensor(3.6410)\n",
      "Epoch  178 Sharpe:  tensor(3.6504)\n",
      "Epoch  179 Sharpe:  tensor(3.6544)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-62f1f129932a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalized_asset_returns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mFt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mreturns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmiu\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mFt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0masset_returns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mFt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mexpected_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(max_iter):\n",
    "    optimizer.zero_grad()\n",
    "    Ft = torch.zeros(T).to(normalized_asset_returns.device)\n",
    "    for i in range(1, T):\n",
    "        data = normalized_asset_returns[i-1:i+M-1]\n",
    "        input = data.view(1,M,1).to(device)\n",
    "        Ft[i] = model(input)\n",
    "    returns = miu * (Ft[:T-1] * asset_returns[M:M+T-1]) - (delta * torch.abs(Ft[1:] - Ft[:T-1]))\n",
    "    expected_return = torch.mean(returns, dim=-1)\n",
    "    std_return = torch.std(returns, dim=-1)\n",
    "    sharpe = expected_return / (torch.sqrt(std_return) + eps)\n",
    "    (-1 * sharpe).backward()\n",
    "    optimizer.step()\n",
    "    rewards.append(sharpe.detach().cpu())\n",
    "    Cum_return_train = returns.cumsum(dim=-1)\n",
    "    print(\"Epoch \", epoch, \"Sharpe: \", sharpe.detach().cpu())\n",
    "# print('Cum_return_train',Cum_returns[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cum_return_train',Cum_return_train[-1])\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [7, 3]\n",
    "# plt.title(\"sharp's ratio optimization RRL\")\n",
    "plt.plot(rewards)\n",
    "# plt.ylim((-0.5,0.5))\n",
    "# plt.legend(['sharp\\'s ratio(RRL):M=10,layer=3'],loc='upper left')\n",
    "plt.savefig(\"SRRRL15-3.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15, 8]\n",
    "f, axes = plt.subplots(3, 1, sharex=True)\n",
    "# plt.suptitle(\"BTC RRL Train\", fontsize=16)\n",
    "\n",
    "axes[0].set_ylabel(\"Prices\")\n",
    "axes[0].plot(prices[M:M+T-1])\n",
    "axes[1].set_ylabel(\"Strategy Returns\")\n",
    "axes[1].plot(Cum_return_train.detach().numpy())\n",
    "axes[2].set_ylabel(\"Ft\")\n",
    "axes[2].bar(list(range(len(Cum_return_train))), Ft[:-1].detach().numpy())\n",
    "# plt.legend(['Train(RRL):M=10,layer=3'],loc='upper left')\n",
    "plt.savefig(\"TrainRRL10-3.png\", dpi=300)\n",
    "plt.show()\n",
    "# print(len(prices[M:M+T]),len(Ft[:-1].detach().numpy()),len(Cum_return_train.detach().numpy()))\n",
    "df_sharp_train_10_3 = pd.DataFrame({'sharp_rrl_10_3':np.array(rewards)})\n",
    "df_sharp_train_10_3.to_csv(r\"./train/sharp_train_rrl_10_3.csv\",sep=',')\n",
    "df_rrl_train_10_3 = pd.DataFrame({'Prices_10_3':prices[M:M+T-1],'Strategy_Returns_10_3':Cum_return_train.detach().numpy(),'F_t_10_3':Ft[:-1].detach().numpy()})\n",
    "df_rrl_train_10_3.to_csv(r\"./train/rrl_train_10_3.csv\",sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prices_test = np.array(raw_prices[1])[OFFSET+M+T:]\n",
    "asset_returns_test = asset_returns[:M+N+1]\n",
    "scaler = StandardScaler()\n",
    "normalized_asset_returns_test = torch.tensor(scaler.fit_transform(asset_returns_test[:M+N+1][:, None])[:, 0]).to(torch.float32)\n",
    "# normalized_asset_returns_test = normalized_asset_returns[M+T:T+M+N+1]\n",
    "# print(asset_returns_test,normalized_asset_returns_test)\n",
    "\n",
    "rewards_test = []\n",
    "Ft_test = torch.zeros(N).to(normalized_asset_returns_test.device)\n",
    "for i in range(1, N):\n",
    "    data_test = normalized_asset_returns_test[i-1:i+M-1]\n",
    "    input_test = data_test.view(1,M,1).to(device)\n",
    "    Ft_test[i] = model(input_test)\n",
    "    returns_test = miu * (Ft_test[:N-1] * asset_returns_test[M:M+N-1]) - (delta * torch.abs(Ft_test[1:] - Ft_test[:N-1]))\n",
    "    expected_return_test = torch.mean(returns_test, dim=-1)\n",
    "    std_return_test = torch.std(returns_test, dim=-1)\n",
    "    sharpe_test = expected_return_test / (torch.sqrt(std_return_test) + eps)\n",
    "    rewards_test.append(sharpe_test.detach().cpu())\n",
    "    Cum_return_tests = returns_test.cumsum(dim=-1)\n",
    "# print('rewards_test',rewards_test)\n",
    "print('Cum_return_test',Cum_return_tests[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 8]\n",
    "f, axes = plt.subplots(4, 1, sharex=True)\n",
    "axes[0].set_ylabel(\"Prices\")\n",
    "axes[0].plot(prices_test[M:M+N+1])\n",
    "axes[1].set_ylabel(\"Strategy Returns\")\n",
    "axes[1].plot(Cum_return_tests.detach().numpy())\n",
    "axes[2].set_ylabel(\"Sharpe ratio\")\n",
    "axes[2].plot(rewards_test)\n",
    "axes[3].set_ylabel(\"Ft\")\n",
    "axes[3].bar(list(range(len(Cum_return_tests))), Ft_test[:-1].detach().numpy())\n",
    "plt.savefig(\"TestRRL10-3.png\", dpi=300)\n",
    "plt.show()\n",
    "df_rrl_test_10_3 = pd.DataFrame({'Prices_10_3':prices_test[M:M+N-1],'Strategy_Returns_10_3':Cum_return_tests.detach().numpy(),'F_t_10_3':Ft_test[:-1].detach().numpy()})\n",
    "df_rrl_test_10_3.to_csv(r\"./test/rrl_test_10_3.csv\",sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
